# Test Manager Documentation

> **Auto-generated index for Test Manager features and documentation.**

**Last Updated**: 2026-01-27 20:51:09

**Total Features**: 2

---

## Crawled Documentation Index

## Features

| # | Feature | Description | Link |
|---|---------|-------------|------|
| 1 | [Link Jira Issues with Test Manager](link-jira-issues-with-test-manager.md) | Test Manager enhances its functionality by seamlessly linking test cases with Jira issues. This inte... | [Source](https://www.testmuai.com/support/docs/link-jira-issues-with-test-manager) |
| 2 | [Test Reporting & Test Management Tools](integrate-test-reporting-test-management-tools.md) | TestMu AI offers integration with test reporting and test management tools, allowing you to convenie... | [Source](https://www.testmuai.com/support/docs/integrate-test-reporting-test-management-tools) |

---

## Quick Links

- [Link Jira Issues with Test Manager](link-jira-issues-with-test-manager.md)
- [Test Reporting & Test Management Tools](integrate-test-reporting-test-management-tools.md)

---

## Official Documentation

- [TestMu AI Docs](https://www.testmuai.com/support/docs/)
- [Test Manager Overview](https://www.testmuai.com/support/docs/integrate-test-reporting-test-management-tools)

---

*This documentation was auto-generated by the TestMu AI Context Crawler.*

---

## Comprehensive Feature Documentation

> **See [test-manager-features.md](test-manager-features.md) for detailed feature documentation including:**

| Section | Topics Covered |
|---------|----------------|
| **Test Case Management** | Repository, folders, test types, custom fields, import/export |
| **Test Execution** | Test runs, parallel/sequential execution, scheduling, configurations |
| **AI Test Generation** | Multi-input support, scenario generation, memory layer |
| **Milestones** | Release tracking, progress monitoring |
| **Reports & Analytics** | Execution reports, traceability, insights dashboard |
| **Datasets** | Data-driven testing, parameters, CSV import |

---

# Test Manager Core Features

> **Documentation**: For complete Test Manager documentation, visit [LambdaTest Test Manager](https://www.lambdatest.com/support/docs/test-manager)

## 1. Test Case Management

> **Learn more**: [Creating Projects in Test Manager](https://www.lambdatest.com/support/docs/create-projects)

### Repository Management
- Centralized test case storage
- Hierarchical organization with projects and folders
- Version control for test cases
- Test case metadata tracking
- Search and filter capabilities
- Tag-based organization

### Folder Organization
- Hierarchical test organization
- Two separate folder types:
  - Test Case Folders
  - Test Run Folders
- Nested folder structure (up to N levels)
- Drag-and-drop folder management
- Bulk move operations

### Test Case Types
- **KaneAI Authored**: AI-generated test cases with automation scripts
- **Manual Test Steps**: Traditional test cases with step-by-step instructions
- **BDD Scenarios**: Behavior-Driven Development test cases

### Custom Fields

> **Learn more**: [System and Custom Fields](https://www.lambdatest.com/support/docs/system-and-custom-fields)

- Extensible test case metadata
- Organization-defined custom fields
- Support for various data types (text, dropdown, date, etc.)
- Custom field filtering and reporting

### Import/Export

> **Learn more**:
> - [CSV Import for Test Cases](https://www.lambdatest.com/support/docs/csv-import)
> - [One-Click Migration from TestRail](https://www.lambdatest.com/support/docs/one-click-migration-from-testrail)

- **Import**: CSV import for bulk test case creation
- **Export**: Multiple format exports
  - CSV
  - PDF
  - Excel
  - JSON
- Bulk operations support

### Bulk Operations
- Mass updates and modifications
- Bulk status changes
- Bulk tagging
- Bulk delete/archive
- Bulk move to folders

### Version History
- Track all changes to test cases
- View previous versions
- Compare versions
- Restore previous versions
- Audit trail for changes

## 2. Test Execution Management

> **Learn more**:
> - [Test Run Creation and Management](https://www.lambdatest.com/support/docs/test-run-creation-and-management)
> - [Automated Test Cases Linking (Dashboard)](https://www.lambdatest.com/support/docs/automated-test-cases-linked-using-dashboard)
> - [Automated Test Cases Linking (Capability)](https://www.lambdatest.com/support/docs/automated-test-cases-linked-using-capability)
> - [Central Configuration Management](https://www.testmu.ai/support/docs/test-runs-configurations)

### Central Configuration Management

**Manage shared configurations across projects from a central location**

- **Centralized Management**: Create and manage test run configurations from a single place
- **Cross-Project Consistency**: Ensure consistency across all test runs in your projects
- **Edit from Central Place**: Update shared configurations that are used across multiple projects
- **Configuration Reusability**: Define once, use across multiple test runs
- **Version Control**: Track changes to configurations over time
- **Use Cases**:
  - Standardize browser/device matrices across teams
  - Maintain consistent environment settings
  - Centralize network throttling configurations
  - Manage retry policies organization-wide

### Test Runs
- Organized test execution campaigns
- Two types:
  - **KaneAI Generated**: Automated execution via HyperExecute
  - **Non-KaneAI Generated**: Manual execution tracking
- Configuration options:
  - Browser/Device matrix
  - Parallel execution (concurrency)
  - Environment selection
  - Network throttling
  - Retry mechanisms
  - **Shared Configurations**: Link to centrally managed configurations

### Test Plans
- Structured testing strategies
- Group related test runs
- Track testing progress
- Milestone association

### Build Management
- Version-specific test execution
- Track test results per build
- Build comparison and analysis
- Release quality tracking

### Parallel Execution
- Concurrent test running
- Configurable concurrency levels
- Resource allocation optimization
- Faster feedback cycles

### Sequential Execution

> **Learn more**: [Sequential Execution in KaneAI Test Runs](https://app.trupeer.ai/view/JsmXsE7Vb) (Video walkthrough)

**Define and control the exact order in which test cases run**

- **Ordered Test Execution**: Specify the precise sequence in which test cases should execute
- **Dependency Management**: Handle test cases with dependencies on previous test results
- **Flow-Based Testing**: Ideal for testing multi-step user journeys that must occur in order
- **Use Cases**:
  - E-commerce checkout flows (browse → add to cart → checkout → payment → confirmation)
  - User onboarding workflows (registration → email verification → profile setup → first login)
  - Multi-stage form submissions
  - Sequential API workflows where each step depends on the previous
  - Integration testing scenarios with ordered prerequisites
- **Configuration**:
  - Drag and drop to reorder test cases
  - Visual sequence indicator
  - Execution follows defined order strictly
  - Stop-on-failure option to halt sequence on first error
- **Benefits**:
  - Ensures correct execution order for dependent tests
  - Reduces flakiness in flow-based scenarios
  - Provides predictable test execution patterns
  - Better debugging by isolating failures in sequence

### Scheduled Executions
- Automated test scheduling
- Recurring schedules (cron-based)
- Timezone configuration
- Pre-scheduled test runs
- CI/CD integration

### Test Execution Workflow

#### Manual Test Execution Flow
1. **Test Run Creation**: Select non-KaneAI test cases
2. **Configuration**: Set up test run metadata
3. **Assignment**: Assign testers
4. **Execution**: Testers execute tests manually
5. **Results**: Mark steps as Passed/Failed
6. **Reporting**: Generate execution reports

#### Automated Test Execution Flow
1. **Test Run Creation**: Select KaneAI authored test cases
2. **Configuration**: Set execution parameters
3. **Trigger**: Manual, scheduled, or API-triggered
4. **Execution**: HyperExecute orchestration
5. **Monitoring**: Real-time execution tracking
6. **Results**: Automated pass/fail status and artifacts

## 3. AI Test Generation

> **Learn more**:
> - [Generate Multiple Tests with AI](https://www.lambdatest.com/support/docs/generate-multiple-tests-with-ai)
> - [KaneAI Test Plan](https://www.lambdatest.com/support/docs/kane-ai-test-plan)

### Multi-Input Support
Generate test cases from various sources:
- **Text**: Plain text descriptions
- **Images**: Screenshots and mockups
- **Audio**: Voice inputs (microphone input)
- **Documents**: PDFs, Word docs (file upload)
- **Third-Party**: Jira issues, Azure DevOps work items (linking)

### Input Settings
The AI Test Case Generator supports several input configuration options:
- **Microphone Input**: Record voice descriptions for test generation
- **File Upload**: Upload documents, images, and screenshots as input
- **Jira/ADO Linking**: Link Jira issues or Azure DevOps work items as input source
- **Maximum Scenario Limit**: Users can set the maximum number of scenarios to generate (configurable up to 20 scenarios maximum)

### Scenario Generation
- Automatic test scenario creation
- 2-10 scenarios per generation request
- Scenario categorization:
  - Must-have (critical)
  - Should-have (important)
  - Could-have (optional)

### Test Categorization
- **Must-have**: Critical scenarios that must be tested
- **Should-have**: Important scenarios recommended for testing
- **Could-have**: Optional scenarios for comprehensive coverage

### Test Types
- **Functional Tests**: Generated for most scenarios
- **Non-Functional Tests**: Performance, security, accessibility (last scenario)
- **Positive Tests**: Happy path scenarios
- **Negative Tests**: Error and edge case scenarios
- **Edge Cases**: Boundary and unusual scenarios

### Third-Party Integration
- Generate from Jira issues
- Generate from Azure DevOps work items
- Automatic linking to source tickets
- Bidirectional traceability

### Test Generation Flow
1. **Input**: Select input source and provide content
2. **Processing**: AI analyzes and generates scenarios
3. **Review**: Review and edit generated test cases
4. **Selection**: Choose which test cases to create
5. **Creation**: Two options:
   - **Create Test Cases**: Save as manual test cases
   - **Create & Automate**: Automatically author with KaneAI agents

### Memory Layer (AI Context Enhancement)

The Memory Layer provides contextual intelligence to the AI Test Case Generator, enabling more relevant and non-duplicate test case generation.

#### Memory Layer Components

**1. Project-Level Instructions**
- Custom instructions set at the project level
- Provide domain-specific context for test generation
- Examples:
  - "This is an e-commerce application with cart functionality"
  - "Always include accessibility test cases"
  - "Focus on mobile-first test scenarios"
- Applied to all test generations within the project

**2. Organization-Level Instructions**
- Custom instructions set at the organization level
- Provide company-wide testing standards and context
- Examples:
  - "All tests must include security validations"
  - "Follow WCAG 2.1 guidelines for accessibility"
  - "Include performance baseline assertions"
- Applied across all projects in the organization
- Project-level instructions can override or extend organization instructions

**3. Memory Enhancement (Repository Context)**
- **Purpose**: Generate non-duplicate test cases by learning from existing test repository
- **How it works**:
  1. User enables "Memory Enhancement" toggle in AI Test Generator
  2. System fetches existing test cases from the current project
  3. Context is extracted from existing test cases:
     - Test case titles and descriptions
     - Test step patterns
     - Coverage areas already tested
  4. LLM uses this context to:
     - Avoid generating duplicate test cases
     - Identify gaps in current test coverage
     - Generate complementary test scenarios
- **Benefits**:
  - Reduces duplicate test case creation
  - Improves test coverage completeness
  - Learns from existing testing patterns
  - Maintains consistency with existing test suite

#### Memory Layer Configuration

```
Organization Settings
└── AI Instructions (organization-level context)

Project Settings
└── AI Instructions (project-level context)

AI Test Generator UI
├── Project Instructions (displayed/editable)
├── Organization Instructions (displayed)
└── Memory Enhancement Toggle
    └── When enabled: Fetches existing test cases for context
```

#### Best Practices
- Keep project instructions specific to the application under test
- Use organization instructions for company-wide standards
- Enable Memory Enhancement for mature projects with existing test suites
- Review and update instructions periodically as the application evolves

## 4. Milestones

> **Learn more**: [Milestone Creation and Management](https://www.lambdatest.com/support/docs/milestone-creation-and-management)

### Purpose
- Central organizational tool for grouping test runs
- Track feature launches and releases
- Monitor testing progress
- Aggregate results across test runs

### Milestone States
- **Active**: Currently in progress
- **Completed**: Finished and marked as complete

### Milestone Attributes
- Name (required)
- Description
- Tags
- Owner (assigned team member)
- Duration (expected timeframe)
- Attachments
- Associated test runs

### Use Cases
- Release tracking
- Sprint testing
- Feature launch monitoring
- Regression testing campaigns

## 5. Reports & Analytics

> **Learn more**:
> - [Insights Dashboard](https://www.lambdatest.com/support/docs/insights-dashboard)
> - [Test Manager Reports](https://www.lambdatest.com/support/docs/tms-reports)

### Execution Reports
- Analyze test execution results
- Identify patterns and trends
- Pass/fail metrics
- Execution time analysis
- Flaky test identification
- Historical trend analysis

### Traceability Reports
- Show alignment between test cases and requirements
- Link test cases to defects
- Requirements coverage
- Gap analysis
- Impact analysis

### Report Types
- Execution Reports
- Traceability Reports
- Custom Reports (with filters)

### Report Features
- **Filters**: Test runs, dates, metadata
- **Recurring Reports**: Automated email delivery
- **Recipients**: Configure email recipients
- **Schedule Frequency**: Daily, weekly, monthly
- **Export**: PDF, CSV, Excel

### Report Configuration
- Name and description
- Report type selection
- Filter criteria:
  - Test runs
  - Date ranges
  - Test metadata
  - Issue metadata
  - Run metadata
- Milestone association
- Recurring schedule setup

### Insights & Historical Reports

> **Learn more**: [Insights Dashboard](https://www.lambdatest.com/support/docs/insights-dashboard)

Test Manager provides a dedicated Insights section with historical reporting capabilities:

#### Insights Dashboard Features
- **Report View**: Visual dashboard showing testing metrics and trends
- **Download Options**: Export reports in multiple formats (PDF, CSV, Excel)
- **Advanced Filters**: Filter by date range, test runs, tags, and custom criteria
- **Email Reports**: Send reports directly via email
- **Recurring Reports**: Schedule automated recurring report delivery
  - Configure recipients
  - Set frequency (daily, weekly, monthly)
  - Automatic email distribution

#### Historical Data
- Track testing progress over time
- Compare results across different periods
- Identify trends and patterns
- Monitor team productivity and test coverage

## 6. Test Instance Management

> **Learn more**: [Test Instance Audit Logs](https://www.lambdatest.com/support/docs/test-instance-audit-logs)

### Test Instances
- Specific execution of test case + configuration
- Track individual test results
- Capture execution artifacts:
  - Screenshots
  - Videos
  - Logs
  - Network traces
- Step-level results
- Execution duration tracking
- Failure analysis

### Test Instance Attributes
- Instance ID
- Test case reference
- Execution configuration
- Start/end times
- Duration
- Result status (Passed/Failed/Skipped)
- Artifacts and logs

## 7. Datasets

### Purpose
- Manage structured data for data-driven testing
- Reuse input values across test cases
- Support multiple test scenarios with different data

### Dataset Types
- **Default Dataset**: Auto-generated during test authoring (immutable)
- **Custom Dataset**: User-created or copied from default (editable)

### Dataset Features
- Parameters (columns/placeholders)
- Rows (test scenarios with different data values)
- Data types (text, numbers, dates, etc.)
- Version history
- Creation methods:
  - Manual data entry
  - AI Autofill
  - CSV Import

### Dataset Usage
- Link datasets to test cases
- Map parameters to dataset columns
- Execute test cases with different data rows
- Data-driven test execution